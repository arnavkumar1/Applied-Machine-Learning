{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"spam.data\",header=None,delimiter=r\"\\s+\")\n",
    "df=shuffle(df)\n",
    "name=df.values\n",
    "x=df[df.columns[:-1]].values\n",
    "y=df[df.columns[-1]].values\n",
    "avg=pd.read_csv(\"threshold.csv\",delimiter=r\"\\s+\",header=None)\n",
    "avg=avg.values.tolist()\n",
    "f = []       \n",
    "feat = [[name[i][j] for i in range(len(name))] for j in range(len(name[0]))]\n",
    "avg2 = []\n",
    "for i in avg:\n",
    "    avg2.append(i[0])\n",
    "for i in range(len(feat)):\n",
    "     f.append(list(map(lambda x : 0 if float(x) < avg2[i] else 1, feat[i])))\n",
    "\n",
    "f = np.transpose(f)\n",
    "a=f[:,:56]\n",
    "b=f[:,57]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(a, b, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Decision Tree containing all functions to run random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy :  0.7074583635047067  \n",
    "#### CPU times: user 50.2 s, sys: 84 ms, total: 50.3 s  \n",
    "#### Wall time: 50.3 s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7074583635047067\n",
      "CPU times: user 50.2 s, sys: 84 ms, total: 50.3 s\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        import csv\n",
    "        import numpy\n",
    "        import math\n",
    "        self.csv = csv\n",
    "        self.np = numpy\n",
    "        self.math = math\n",
    "        self.n = Node() \n",
    "    def entropy(self, l):\n",
    "        entropy = 0\n",
    "        for i in range(len(l)):\n",
    "            total = l[i][0] + l[i][1]\n",
    "            prob1 = l[i][0]\n",
    "            prob2 = l[i][1]\n",
    "            if(prob1 == 0 or prob2 == 0):\n",
    "                entropy += 0\n",
    "            else:\n",
    "                p1 = float(prob1)/total\n",
    "                p2 = float(prob2)/total\n",
    "                entropy += (total*(-p1*np.log2(p1) - p2*np.log2(p2)))\n",
    "        return entropy\n",
    "    def random_forest(self,m,n_trees,X_train,X_test):\n",
    "        prediction=[]\n",
    "        for i in range(n_trees): \n",
    "            att = []\n",
    "            count1 = 0\n",
    "            while(count1 != m):\n",
    "                p = random.randrange(0,len(X_train[0])-1)\n",
    "                if(p not in att):\n",
    "                    att.append(p)\n",
    "                    count1 += 1\n",
    "            att.append(len(X_train[0])-1)\n",
    "            f_1 = X_test[:,[x for x in att]]\n",
    "            f_2 = X_train[:,[x for x in att]]\n",
    "            self.train(f_2)\n",
    "            pred = self.predict(f_1)\n",
    "            prediction.append(pred)\n",
    "\n",
    "        temp1 = np.transpose(np.array(prediction))\n",
    "        temp1.tolist()\n",
    "        final_pred = []\n",
    "\n",
    "        for i in range(len(temp1)):\n",
    "            c = Counter(temp1[i])\n",
    "            value,count= c.most_common()[0]\n",
    "            # print(value,count)\n",
    "            final_pred.append(value)\n",
    "\n",
    "\n",
    "        ascore=0\n",
    "        for i in range(len(y_test)):\n",
    "            if(final_pred[i] == y_test[i]):\n",
    "                ascore+=1\n",
    "\n",
    "        accuracy = float(ascore)/len(final_pred)\n",
    "        print(\"Accuracy : \" ,accuracy)\n",
    "        #print(\"Accuracy:\",metrics.accuracy_score(y_test, temp1))\n",
    "    def predict(self, data):\n",
    "        f = [self.pr(i) for i in data]\n",
    "        return f\n",
    "    def modify(self, dataset, newf, l, data):\n",
    "        #Dataset at a node is divided on among its child nodes\n",
    "        for i in dataset:\n",
    "            x = data[i][newf]\n",
    "            y = data[i][len(data[0])-1]\n",
    "            l[x].append(i)\n",
    "            l[x][y] += 1\n",
    "    def pr(self, l):\n",
    "        # Given test data, go to the root of the decision tree. Compare splitting feature with corresponding value\n",
    "        # Go to appropriate child node. Keep on doing this until a leaf node is encountered\n",
    "        temp = self.n\n",
    "        feat = temp.feature\n",
    "        while feat!=-1:\n",
    "            temp = temp.children[(l[feat])]\n",
    "            feat = temp.feature\n",
    "        # If leaf has no entry, go to parent node.\n",
    "        while temp.yes + temp.no == 0:\n",
    "            temp = temp.parent\n",
    "        # If no of yes is more, classify in class 1. Else in class 0\n",
    "        if temp.yes > temp.no:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def run(self, node, unassign, datasets, feature, data):\n",
    "        unassigned = unassign[:]\n",
    "        dataset = datasets[:]\n",
    "        for i in dataset:\n",
    "            if data[i][len(data[0])-1] == 1:\n",
    "                node.yes += 1\n",
    "            else:\n",
    "                node.no += 1\n",
    "        # Breaking if no more dataset/feature to split or entropy is low\n",
    "        if unassigned == [] or dataset == []:\n",
    "            return\n",
    "        x = unassigned[0]\n",
    "        l = [[0, 0] for _ in range(feature[x])]\n",
    "        self.modify(dataset, x, l, data)\n",
    "        et = self.entropy(l)\n",
    "        \n",
    "        m = []\n",
    "        for i in unassigned[1:]:\n",
    "            m = [[0, 0] for _ in range(feature[i])]\n",
    "            self.modify(dataset, i, m, data) \n",
    "            temp = self.entropy(m)      \n",
    "            if(temp < et):\n",
    "                et = temp\n",
    "                x = i\n",
    "                l = m\n",
    "\n",
    "        node.feature = x\n",
    "        unassigned.remove(x)\n",
    "\n",
    "        # Creating children node. # = no of values splitting feature can take\n",
    "        for i in range(len(l)):\n",
    "            child = Node()\n",
    "            child.parent = node\n",
    "            node.add_child(child)\n",
    "        # Recursively calling the function\n",
    "        for i in range(len(l)):\n",
    "            self.run(node.children[i], unassigned, l[i][2:],feature,data)\n",
    "    def train(self, data):\n",
    "        unassign = [x for x in range(len(data[0])-1)]\n",
    "        dataset = [x for x in range(len(data))]\n",
    "        feature = [2 for i in range(len(data[0])-1)]\n",
    "        self.run(self.n, unassign, dataset, feature, data)\n",
    "def main():\n",
    "    m=31\n",
    "    n_trees = 19\n",
    "    model = DecisionTree()\n",
    "    model.random_forest(m,n_trees,X_train,X_test)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self):\n",
    "        self.feature = -1\n",
    "        self.children = []\n",
    "        self.yes = 0\n",
    "        self.no = 0\n",
    "        self.parent = -1\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy: 0.943519188993483  \n",
    "#### CPU times: user 252 ms, sys: 0 ns, total: 252 ms  \n",
    "#### Wall time: 251 ms  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.939174511223751\n",
      "Accuracy: 0.942795076031861\n",
      "Accuracy: 0.942070963070239\n",
      "Accuracy: 0.944243301955105\n",
      "Accuracy: 0.941346850108617\n",
      "Accuracy: 0.939174511223751\n",
      "Accuracy: 0.941346850108617\n",
      "Accuracy: 0.942070963070239\n",
      "252 ms ± 12.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(bootstrap=True,n_estimators=100)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
